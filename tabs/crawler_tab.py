"""ç´ æè‡ªå‹•æ¡é›†åˆ†é ï¼šæ•´åˆ gallery-dl"""
import re
import subprocess
import threading
import shutil
from pathlib import Path
from urllib.parse import quote_plus

import gradio as gr

from . import shared

_PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = _PROJECT_ROOT / "data"
DEFAULT_CRAWL_DIR = DATA_DIR / "temp_raw"
DANBOORU_BASE = "https://danbooru.donmai.us/posts?tags="

# æ•¸é‡é¸é …ï¼š(é¡¯ç¤ºæ–‡å­—, å¯¦éš› range å€¼)
RANGE_OPTIONS = [
    ("50 å¼µ", "1-50"),
    ("50-100 å¼µ", "1-100"),
    ("100-200 å¼µ", "1-200"),
    ("200-300 å¼µ", "1-300"),
    ("500 å¼µ", "1-500"),
]
RANGE_MAP = {label: val for label, val in RANGE_OPTIONS}

# å…è¨±ä¸‹è¼‰çš„åœ–ç‰‡æ ¼å¼
ALLOWED_EXTENSIONS = ("jpg", "jpeg", "png", "webp")
# æ’é™¤çš„æ ¼å¼ï¼ˆå½±ç‰‡ã€å‹•åœ–ç­‰ï¼‰
EXCLUDED_EXTENSIONS = ("mp4", "webm", "gif", "avi", "mov", "mkv")
_SKIP_PATTERN = re.compile(
    r"[^/\\]+\.(" + "|".join(re.escape(e) for e in EXCLUDED_EXTENSIONS) + r")(?:\?|$|\s|\))",
    re.IGNORECASE,
)

# ä¾›åœæ­¢æŒ‰éˆ•ä½¿ç”¨
_crawl_proc = None
_crawl_stopped = False


def stop_crawl(current_log: str = "") -> str:
    """åœæ­¢ç•¶å‰æŠ“å–ä»»å‹™ï¼Œproducer æœƒåµæ¸¬ä¸¦å¯«å…¥æ—¥èªŒ"""
    global _crawl_proc, _crawl_stopped
    _crawl_stopped = True
    if _crawl_proc and _crawl_proc.poll() is None:
        try:
            _crawl_proc.terminate()
        except Exception:
            pass
    return current_log or ""


def check_gallery_dl() -> tuple[bool, str]:
    """æª¢æŸ¥ gallery-dl æ˜¯å¦å·²å®‰è£ï¼Œå›å‚³ (æ˜¯å¦å¯ç”¨, æç¤ºè¨Šæ¯)"""
    exe = shutil.which("gallery-dl")
    if exe:
        return True, f"âœ… gallery-dl å·²å®‰è£: {exe}"
    try:
        import gallery_dl  # noqa: F401
        return True, "âœ… gallery-dl å·²å®‰è£ (python -m gallery_dl)"
    except ImportError:
        pass
    return False, (
        "âš ï¸ æœªåµæ¸¬åˆ° gallery-dlï¼Œè«‹å…ˆå®‰è£ï¼š pip install gallery-dl\n"
        "https://github.com/mikf/gallery-dl#installation"
    )


def _run_crawler_stream(tags, range_val, output_dir, sleep_sec, chain_wd14, wd14_trigger, sort_by,
                        helper_ref_dest, helper_triggers):
    """åŸ·è¡Œ gallery-dl ä¸‹è¼‰ï¼Œå¯é¸é€£é– WD14 + ç´ æç¯©é¸"""
    global _crawl_proc, _crawl_stopped
    if not tags or not tags.strip():
        yield "âŒ è«‹è¼¸å…¥é—œéµå­— (Tags)"
        return

    ok, msg = check_gallery_dl()
    if not ok:
        yield msg
        return

    output_path = Path(output_dir or str(DEFAULT_CRAWL_DIR)).resolve()
    output_path.mkdir(parents=True, exist_ok=True)

    tags_encoded = quote_plus(tags.strip())
    url = DANBOORU_BASE + tags_encoded

    try:
        sleep_val = max(0, float(sleep_sec)) if sleep_sec is not None else 1.0
    except (TypeError, ValueError):
        sleep_val = 1.0

    # å„ªå…ˆä½¿ç”¨ PATH ä¸­çš„ gallery-dlï¼Œå¦å‰‡ç”¨ python -m gallery_dl
    if shutil.which("gallery-dl"):
        cmd_base = ["gallery-dl"]
    else:
        cmd_base = ["python", "-m", "gallery_dl"]

    # åƒ…ä¸‹è¼‰ jpg/jpeg/png/webpï¼Œæ’é™¤ mp4/webm/gif ç­‰
    ext_filter = f"extension.lower() in {ALLOWED_EXTENSIONS}"
    size_filter = "image_width > 512 and image_height > 512"
    combined_filter = f"{ext_filter} and {size_filter}"

    cmd = cmd_base + [
        "--range", range_val or "1-100",
        "--directory", str(output_path),
        "--sleep", str(sleep_val),
        "--filter", combined_filter,
        "--verbose",
        url,
    ]

    def producer(log_cb):
        global _crawl_proc, _crawl_stopped
        _crawl_stopped = False
        _crawl_proc = None

        log_cb(f"ğŸš€ åŸ·è¡Œ: {' '.join(cmd)}")
        log_cb("ğŸ“· åƒ…ä¸‹è¼‰ jpg/jpeg/png/webpï¼Œæ’é™¤ mp4/webm/gif ç­‰å½±ç‰‡èˆ‡å‹•åœ–")
        log_cb("")

        proc = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            encoding="utf-8",
            errors="replace",
        )
        _crawl_proc = proc

        def _read():
            for line in iter(proc.stdout.readline, ""):
                if not line:
                    continue
                line = line.rstrip()
                m = _SKIP_PATTERN.search(line)
                if m and ("skip" in line.lower() or "filter" in line.lower() or "filtered" in line.lower()):
                    log_cb(f"å·²è·³ééåœ–ç‰‡è³‡æº: {m.group(0)}")
                else:
                    log_cb(line)
            proc.stdout.close()

        t = threading.Thread(target=_read, daemon=True)
        t.start()
        proc.wait()
        _crawl_proc = None

        if _crawl_stopped:
            log_cb("\nâ¹ å·²æ‰‹å‹•åœæ­¢æŠ“å–")
            return
        if proc.returncode != 0:
            log_cb(f"\nâš ï¸ gallery-dl çµæŸç¢¼: {proc.returncode}")
            return

        log_cb("\nâœ… ä¸‹è¼‰å®Œæˆ")

        if not chain_wd14:
            return

        # é€£é–ï¼šWD14 æ¨™è¨»
        log_cb("\n" + "=" * 50)
        log_cb("ğŸ”„ è‡ªå‹•åŸ·è¡Œ WD14 æ¨™è¨»...")
        from src.tagger_wd14 import tag_folder as wd14_tag_folder
        try:
            n = wd14_tag_folder(
                str(output_path),
                trigger_word=wd14_trigger or "Niyaniya",
                sort_by_category=sort_by,
                log_callback=log_cb,
            )
            log_cb(f"\nâœ… WD14 æ¨™è¨»å®Œæˆï¼Œè™•ç† {n} å¼µåœ–ç‰‡")
        except Exception as e:
            log_cb(f"\nâŒ WD14 æ¨™è¨»å¤±æ•—: {e}")
            return

        # é€£é–ï¼šç´ æç¯©é¸ (helper_grabber)
        log_cb("\n" + "=" * 50)
        log_cb("ğŸ”„ è‡ªå‹•åŸ·è¡Œç´ æç¯©é¸...")
        from src.helper_grabber import grab_hand_feet_refs
        try:
            dest = helper_ref_dest or str(output_path.parent / "crawler_filtered")
            triggers = [t.strip() for t in (helper_triggers or "").split(",") if t.strip()] or ["Niyaniya", "Ibuki"]
            n = grab_hand_feet_refs(
                str(output_path),
                dest,
                trigger_words=triggers,
                recursive=True,
                log_callback=log_cb,
            )
            log_cb(f"\nâœ… ç´ æç¯©é¸å®Œæˆï¼Œè¤‡è£½ {n} å¼µåœ–ç‰‡è‡³ {dest}")
        except Exception as e:
            log_cb(f"\nâŒ ç´ æç¯©é¸å¤±æ•—: {e}")

    for text in shared.stream_from_log_callback(producer):
        yield text


def render(defaults: dict, wd14_comps: dict, train_comps: dict):
    """
    å»ºç«‹ç´ æè‡ªå‹•æŠ“å–åˆ†é ã€‚
    éœ€å¾ WD14ã€Train åˆ†é å–å¾—ï¼šsort_tags_by_category, wd14_trigger_word, helper_ref_dest, helper_trigger_words
    """
    ok, env_msg = check_gallery_dl()
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown(f"**ç’°å¢ƒæª¢æŸ¥**: {env_msg}")
            tags_input = gr.Textbox(
                label="é—œéµå­— (Tags)",
                value=defaults.get("crawler_tags", "hand_focus rating:g score:>20"),
                placeholder="Danbooru æ¨™ç±¤ï¼Œå¦‚ hand_focus rating:g score:>20",
                lines=2,
            )
            range_dropdown = gr.Dropdown(
                label="æ•¸é‡",
                choices=[label for label, _ in RANGE_OPTIONS],
                value="50-100 å¼µ",
            )
            sleep_input = gr.Slider(
                label="è«‹æ±‚å»¶é² (ç§’) â€” é¿å…è«‹æ±‚å¤ªé »ç¹è¢«åœ–åº«ç«™å°é–",
                minimum=0,
                maximum=15,
                step=0.5,
                value=defaults.get("crawler_sleep", 1.0),
            )
            with gr.Row():
                output_dir = gr.Textbox(
                    label="è¼¸å‡ºè·¯å¾‘",
                    value=defaults.get("crawler_output_dir", str(DEFAULT_CRAWL_DIR)),
                    placeholder="ä¸‹è¼‰è‡³...",
                    scale=9,
                )
                gr.Button("ç€è¦½...", scale=1).click(
                    fn=lambda x: shared.browse_folder(x),
                    inputs=[output_dir],
                    outputs=[output_dir],
                )
            chain_check = gr.Checkbox(
                label="ä¸‹è¼‰å®Œå¾Œè‡ªå‹•åŸ·è¡Œ WD14 æ¨™è¨»èˆ‡ç´ æç¯©é¸",
                value=defaults.get("crawler_chain_wd14", False),
            )
            with gr.Row():
                crawl_btn = gr.Button("é–‹å§‹ä¸‹è¼‰", variant="primary")
                stop_btn = gr.Button("åœæ­¢æŠ“å–")

    # ä¾è³´å…¶ä»–åˆ†é çš„å…ƒä»¶ï¼ˆç”¨æ–¼é€£é–æ™‚å–å¾—åƒæ•¸ï¼‰
    wd14_trigger = wd14_comps.get("wd14_trigger_word")
    sort_by = wd14_comps.get("sort_tags_by_category")
    helper_ref_dest = train_comps.get("helper_ref_dest")
    helper_triggers = train_comps.get("helper_trigger_words")

    def _run(tags, range_label, out_dir, sleep_sec, chain, trig, sb, ref_dest, triggers):
        range_val = RANGE_MAP.get(range_label, "1-100")
        for text in _run_crawler_stream(
            tags, range_val, out_dir, sleep_sec, chain,
            trig, sb, ref_dest, triggers
        ):
            yield text

    bindings = [
        (crawl_btn, _run, [
            tags_input, range_dropdown, output_dir, sleep_input, chain_check,
            wd14_trigger, sort_by, helper_ref_dest, helper_triggers,
        ]),
    ]

    comps = {
        "stop_crawl_btn": stop_btn,
        "stop_crawl_fn": stop_crawl,
        "crawler_tags": tags_input,
        "crawler_output_dir": output_dir,
        "crawler_sleep": sleep_input,
        "crawler_chain_wd14": chain_check,
    }
    return comps, bindings
