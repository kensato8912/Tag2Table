"""
è™•ç†æ¨¡çµ„ï¼šæ•´åˆè³‡æ–™åº«èˆ‡ç¿»è­¯ APIï¼ŒåŸ·è¡Œæ¨™ç±¤æƒæã€ç¿»è­¯ã€åˆ†é¡ã€è¼¸å‡º
"""
from pathlib import Path
from collections import Counter

from .database_manager import (
    load_tag_database, load_tag_map, save_tags_to_json, update_tag_database,
    get_category, scan_txt_files, extract_tags_from_file, tag_map,
    CATEGORIES, TAGS_DB_FILE, DB_FILE,
)

# å‹•æ…‹åˆ†é¡æ’åºæ¬Šé‡ï¼ˆæ„ˆå°æ„ˆé å‰ï¼‰
CATEGORY_SORT_WEIGHTS = {
    "è§’è‰²èˆ‡ä½œå“ (Character)": 1,
    "èº«é«”ç‰¹å¾µ (Body)": 2,
    "è¡¨æƒ… (Expression)": 3,
    "è¡¨æƒ… (Pose)": 3,
    "è¡£æœé…ä»¶ (Clothing)": 4,
    "å§¿æ…‹å‹•ä½œ (Pose)": 5,
    "å…‰ç·šé¢¨æ ¼ (Style)": 6,
    "èƒŒæ™¯ç’°å¢ƒ (Background)": 7,
}
DEFAULT_SORT_WEIGHT = 8


def _normalize_tag_for_lookup(tag: str) -> str:
    """å°‡æ¨™ç±¤æ­£è¦åŒ–ä»¥ä¾¿èˆ‡ JSON æ¯”å°ï¼ˆç§»é™¤è½‰ç¾©ç¬¦è™Ÿï¼‰"""
    return tag.replace("\\(", "(").replace("\\)", ")")


def _build_category_lookup(db_path=None):
    """å¾ all_characters_tags.json å»ºç«‹ æ¨™ç±¤->åˆ†é¡ å°ç…§ï¼Œæ”¯æ´å¤šç¨®æ ¼å¼æ¯”å°"""
    db = load_tag_database(db_path)
    lookup = {}
    if not db:
        return lookup
    for en_tag, item in db.items():
        cat = item.get("category")
        if not cat:
            continue
        lookup[en_tag] = cat
        normalized = _normalize_tag_for_lookup(en_tag)
        if normalized != en_tag:
            lookup[normalized] = cat
        lookup[en_tag.replace("_", " ")] = cat
        lookup[en_tag.replace(" ", "_")] = cat
    return lookup


def sort_tags_by_category(
    tags: list[str],
    db_path=None,
) -> list[str]:
    """
    ä¾ all_characters_tags.json çš„ category æ¬„ä½å°æ¨™ç±¤æ’åºã€‚
    æ¬Šé‡ï¼šè§’è‰²èˆ‡ä½œå“(1) < èº«é«”ç‰¹å¾µ(2) < è¡¨æƒ…(3) < è¡£æœé…ä»¶(4) < å§¿æ…‹å‹•ä½œ(5) < å…‰ç·šé¢¨æ ¼(6) < èƒŒæ™¯ç’°å¢ƒ(7) < å…¶é¤˜(8)
    """
    lookup = _build_category_lookup(db_path)

    def sort_key(tag: str) -> tuple:
        norm = _normalize_tag_for_lookup(tag)
        cat = lookup.get(tag) or lookup.get(norm) or lookup.get(tag.replace("_", " ")) or lookup.get(tag.replace(" ", "_"))
        weight = CATEGORY_SORT_WEIGHTS.get(cat, DEFAULT_SORT_WEIGHT) if cat else DEFAULT_SORT_WEIGHT
        return (weight, tag)

    return sorted(tags, key=sort_key)
from .ollama_client import (
    init_gemini, get_ollama_client, get_ai_translation, get_gemini_translation,
    batch_translate_and_classify_gemini, batch_translate_and_classify_ollama,
)


def process_with_ai(folder_path, ollama_url, model_name, output_file, enable_classification=False,
                    translation_mode="ollama", gemini_api_key="",
                    log_callback=None, progress_callback=None):
    """
    ä¸»è¦è™•ç†å‡½æ•¸
    åƒæ•¸:
        translation_mode: "ollama" æˆ– "gemini"
        gemini_api_key: Gemini API Keyï¼ˆåƒ… gemini æ¨¡å¼éœ€è¦ï¼‰
    """
    def log(msg):
        if log_callback:
            log_callback(msg)
        else:
            print(msg)

    def update_progress(value, max_value):
        if progress_callback:
            progress_callback(value, max_value)

    use_gemini = (translation_mode == "gemini")

    if use_gemini:
        if not gemini_api_key:
            log("âŒ è«‹è¼¸å…¥ Gemini API Key")
            return False
        log("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– Gemini APIï¼ˆæ–°ç‰ˆ google-genai SDKï¼‰...")
        gemini_client, actual_model = init_gemini(gemini_api_key, log_callback=log)
        if gemini_client is None:
            log("âŒ Gemini åˆå§‹åŒ–å¤±æ•—ï¼Œå·²çµ‚æ­¢")
            return False
        client = None
    else:
        log("ğŸ”§ æ­£åœ¨é€£æ¥åˆ° Ollama æœå‹™...")
        client, actual_model = get_ollama_client(ollama_url, model_name, log_callback=log)
        if client is None:
            log("âŒ ç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™")
            log("ğŸ’¡ æç¤ºï¼šè«‹ç¢ºèª Ollama æœå‹™æ­£åœ¨é‹è¡Œï¼ˆåŸ·è¡Œ: ollama serveï¼‰")
            return False

    log("=" * 70)
    log("ğŸš€ é–‹å§‹æƒææ¨™ç±¤æª”...")
    log("=" * 70)

    txt_files = scan_txt_files(folder_path, log_callback=log)
    if not txt_files:
        log("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½• .txt æ¨™ç±¤æª”ï¼")
        return False

    log(f"\nğŸ“Š æ­£åœ¨è®€å–ä¸¦çµ±è¨ˆæ¨™ç±¤...")
    all_tags = []
    for i, txt_file in enumerate(txt_files):
        tags = extract_tags_from_file(txt_file)
        all_tags.extend(tags)
        if (i + 1) % 50 == 0:
            update_progress(i + 1, len(txt_files))

    if not all_tags:
        log("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ¨™ç±¤ï¼")
        return False

    tag_counts = Counter(all_tags)
    total_unique_tags = len(tag_counts)
    log(f"âœ… å…±æ‰¾åˆ° {total_unique_tags} å€‹ä¸é‡è¤‡çš„æ¨™ç±¤")

    db = load_tag_database()
    tag_map_local = dict(tag_map)
    category_map = {}
    if db:
        for en, item in db.items():
            tag_map_local[en] = item.get('zh_tag', '')
            if item.get('category'):
                category_map[en] = item['category']
        log(f"ğŸ“‚ å·²è¼‰å…¥ {len(db)} ç­†æ—¢æœ‰è³‡æ–™ï¼Œæ™ºæ…§è·³éé‡è¤‡ä¸¦æ›´æ–°ã€Œå…¶ä»–ã€é¡")

    OTHER_LIKE = (None, '', 'å…¶ä»– (General)', 'å…¶ä»–')
    VALID_TRANSLATION = lambda t: t and t not in ('', 'æœªç¿»è­¯', 'ï¼ˆæœªç¿»è­¯ï¼‰')
    sorted_tags = [tag for tag, _ in tag_counts.most_common()]

    tags_need_translate = [t for t in sorted_tags
                          if t not in db or not VALID_TRANSLATION(db[t].get('zh_tag'))]
    if enable_classification:
        tags_need_translate = sort_tags_by_category(tags_need_translate)
    tags_reclassify_only = [t for t in sorted_tags
                           if t in db
                           and VALID_TRANSLATION(db[t].get('zh_tag'))
                           and db[t].get('category') in OTHER_LIKE]

    engine = "Gemini" if use_gemini else f"Ollama ({actual_model})"
    log(f"\nğŸŒ æ­£åœ¨ä½¿ç”¨ {engine} ç¿»è­¯æ¨™ç±¤...")
    log("=" * 70)

    if tags_need_translate:
        log(f"ğŸ“¦ éœ€è¦ç¿»è­¯çš„æ¨™ç±¤: {len(tags_need_translate)} å€‹")
        batch_size = 80
        for i in range(0, len(tags_need_translate), batch_size):
            chunk = tags_need_translate[i:i + batch_size]
            if use_gemini:
                batch_result = batch_translate_and_classify_gemini(
                    chunk, gemini_api_key, actual_model, log_callback=log, pre_sorted_hint=enable_classification
                )
            else:
                batch_result = batch_translate_and_classify_ollama(
                    client, actual_model, chunk, log_callback=log, pre_sorted_hint=enable_classification
                )
            for en, data in batch_result.items():
                tag_map_local[en] = data.get('zh_tag', 'ï¼ˆæœªç¿»è­¯ï¼‰')
                if data.get('category'):
                    category_map[en] = data['category']

    if tags_reclassify_only:
        log(f"ğŸ”„ ä¿ç•™ç¿»è­¯ã€é‡æ–°åˆ†é¡çš„æ¨™ç±¤: {len(tags_reclassify_only)} å€‹")

    tag_map.update(tag_map_local)

    def resolve_category(tag):
        kw_cat = get_category(tag)
        if kw_cat != "å…¶ä»– (General)":
            return kw_cat
        return category_map.get(tag) or kw_cat

    def get_local_translation(tag):
        if tag not in tag_map:
            if use_gemini:
                tag_map[tag] = get_gemini_translation(tag, gemini_client, actual_model, log_callback=log)
            else:
                tag_map[tag] = get_ai_translation(tag, client, actual_model, log_callback=log)
        return tag_map[tag]

    try:
        Path(output_file).parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as out:
            if enable_classification:
                classified = {cat: [] for cat in CATEGORIES.keys()}
                classified["å…¶ä»– (General)"] = []
                for tag, count in tag_counts.most_common():
                    category = resolve_category(tag)
                    chinese = get_local_translation(tag)
                    line = f"{tag.ljust(30)} | {str(count).ljust(5)} | {chinese}"
                    classified[category].append(line)
                    log(f"[{category}] {tag} -> {chinese}")
                cat_order = ["è§’è‰²èˆ‡ä½œå“ (Character)"] + [c for c in CATEGORIES.keys() if c != "è§’è‰²èˆ‡ä½œå“ (Character)"] + ["å…¶ä»– (General)"]
                for cat in cat_order:
                    lines = classified.get(cat, [])
                    if lines:
                        out.write(f"\nâ— {cat}\n")
                        out.write("-" * 60 + "\n")
                        out.write("\n".join(lines) + "\n")
            else:
                for tag, count in tag_counts.most_common():
                    chinese = get_local_translation(tag)
                    line = f"{tag.ljust(30)} | {str(count).ljust(5)} | {chinese}\n"
                    out.write(line)
                    log(f"å·²ç¿»è­¯: {tag} -> {chinese}")

        log("=" * 70)
        log(f"âœ¨ å®Œæˆï¼çµæœå·²å„²å­˜è‡³ï¼š{output_file}")
        log(f"ğŸ“ ç¸½å…±è™•ç†äº† {len(txt_files)} å€‹æª”æ¡ˆï¼Œ{total_unique_tags} å€‹ä¸é‡è¤‡æ¨™ç±¤")
        character = Path(folder_path).resolve().parent.name
        new_tags_list = [
            {"en_tag": tag, "zh_tag": tag_map.get(tag, "æœªç¿»è­¯"), "count": count, "category": resolve_category(tag)}
            for tag, count in tag_counts.items()
        ]
        save_tags_to_json(tag_counts, tag_map, TAGS_DB_FILE, log_callback=log, category_getter=resolve_category)
        update_tag_database(new_tags_list, DB_FILE, character=character, log_callback=log)
        log("=" * 70)
        return True

    except RuntimeError as e:
        log(f"âŒ å·²çµ‚æ­¢: {e}")
        return False
    except Exception as e:
        log(f"âŒ å¯«å…¥æª”æ¡ˆå¤±æ•—: {e}")
        return False
